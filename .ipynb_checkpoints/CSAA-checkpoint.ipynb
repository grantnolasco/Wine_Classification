{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Libraries ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import csv\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# === Potential libraries to use === \n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "#from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the directory where the notes and claims data is\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Reading the note_data3.csv to a pandas dataframe\n",
    "notes_old = pd.read_csv('note_data4.csv')\n",
    "#claims = pd.read_csv('claim_data4.csv')\n",
    "\n",
    "# Use a subset to avoid memory error (first hundred thousand values)\n",
    "#n_samples = 100000\n",
    "#notes_subset = notes_old[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First five values of notes_subset\n",
    "notes_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about notes_subset (i.e column types and dimensions)\n",
    "notes_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data for notes for basic exploration\n",
    "text = notes['body']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of notes\n",
    "notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of null values\n",
    "sum(notes['body'].isna() == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the null values\n",
    "notes = notes[notes['body'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text data to all lowercase to avoid multiple copies of the same word \n",
    "notes['body'] = notes['body'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation as it doesn't add any information \n",
    "notes['body'] = notes['body'].str.replace('[^\\w\\s]','')\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Stop Words as it also won't add any information\n",
    "stop_nltk = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "stop_tm = list(pd.read_csv('tm_stop.csv').iloc[:,1])\n",
    "\n",
    "stopWords = list(set().union(stop_nltk, stop_tm))\n",
    "\n",
    "notes['body'] = notes['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopWords))\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "notes['body'] = notes['body'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming \n",
    "st = PorterStemmer()\n",
    "notes['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words using CSAA data after stemming/lemmatization\n",
    "CSAA_stop_words = [\"how\", \"you\", \"absolut\", \"account\", \"actual\", \"address\", \"afternoon\", \"agent\", \"ahead\", \"alreadi\", \"alright\", \"alrighti\", \n",
    "\"also\", \"alway\",\"answer\", \"anyth\", \"apolog\", \"appreci\", \"assist\", \"auto\", \"automat\", \"avail\", \"gonna\", \n",
    "\"good\", \"great\", \"guess\", \"hello\", \"help\", \"hope\",\"just\", \"kind\", \"kinda\", \"know\", \"like\", \"mhmm\",\n",
    "\"might\", \"name\", \"number\", \"phone\",\"pleas\",\"pleasur\",\"polici\",\"probabl\", \"problem\", \"question\", \"quick\",\n",
    "\"welcom\", \"well\", \"went\", \"whatev\", \"whenev\", \"wonder\", \"xxxx\", \"xxxxx\", \"cam\", \"xxxxxx\", \"xxxxxxx\", \"xxxxxxxx\", \n",
    "\"xxxxxxxxx\", \"xxxxxxxxxx\", \"xxxxxxxxxxx\", \"yeah\", \"youd\", \"youll\", \"youv\", \"help\", \"wanna\", \"want\", \"much\", \n",
    "\"today\", \"sure\", \"call\", \"right\", \"xaxx\",  \"xoxxxxx\", \"xxxc\", \"okay\", \"um\", \"uh\", \"yeah\", \"thank\", \"policy\",\n",
    "\"let\", \"yes\", \"mean\", \"moment\", \"don\", \"going\", \"hi\", \"bye\", \"need\", \"calling\", \"oh\", \"ll\", \"hold\",\n",
    "\"day\", \"think\", \"look\", \"got\", \"did\", \"correct\", \"aaa\", \"aaa insurance\", \"dot\", \"com\", \"thanks\", \"holding\",\n",
    "\"br\", \"xx\", \"xxx\", \"appreciate\", \"business\", \"representative\", \"available\", \"sir\", \"ma\", \"michelle\", \"kimberly\",\n",
    "\"nancy\",\"line\", \"morning\", \"actually\", \"gonzalez\", \"minute\", \"welcome\", \"perfect\", \"verify\", \"zip\", \"code\",\n",
    "\"axxxx\", \"verify zip code\", \"verify zip\", \"patience\", \"mister\", \"sorry\", \"exactly\", \"ve\", \"reached\", \"place\",\n",
    "\"minutes\", \"ask\", \"couple\", \"questions\", \"insurance\", \"needs\", \"ma'am\", \">\", \"<\", \"'ll\", \"happy\",\n",
    "\"'m\", \"'re\", \"'s\", \"'re\", \"x\", \"gon\", \"na\", \"n't\", \"na\", \"ca\", \"wan\", \"does\",\n",
    "\"taken\", \"care\", \"little\", \"bit\", \"double\", \"check\", \"maybe\", \"said\", \"say\", \"really\", \"cause\", \"way\",\n",
    "\"date\", \"july\", \"august\", \"year\", \"month\", \"customer\", \"service\", \"recorded\", \"monitored\", \"quality\", \"center\",\n",
    "\"xxxxxxxxxxxxxxxx\", \"best\", \"possible\", \"speaking\", \"bring\", \"able\", \"xxxxxxxxxxxxxxxx\", \"xxxxxxxxxxxxxxxx\",\n",
    "\"apologize\", \"services\", \"providing\", \"doing\", \"able\", \"ready\", \"looks\", \"apologize\", \"provide\", \"speak\", \"doing\",\n",
    "\"absolutely\", \"access\", \"wonderful\", \"information\", \"enjoy\", \"rest\", \"assisting\", \"guy\", \"november\", \"december\", \n",
    "\"somebody\",\"honda\", \"huh\", \"pleasure\", \"choosing\", \"supervisor\", \"evening\", \"waiting\", \"weekend\", \"october\", \"dollar\", \"guys\", \"brief\",\n",
    "\"wait\", \"contact\", \"alrighty\", \"nice\", \"ok\", \"trying\", \"mind\", \"reason\", \"miss\", \"getting\", \"thousand\",\n",
    "\"looking\", \"course\", \"didn\", \"press\", \"ensure\", \"set\", \"loyal\", \"conversation\", \"pull\",\n",
    "\"clmt\", \"clmts\" , \"claimant\"  , \"claimants\"  , \"clmnt\",\"csaa\",  \"mr\" ,  \"wb\", \"nb\", \"sb\", \"eb\"  , \"claim\", \"injury\", \n",
    "\"office\", \"letter\", \"form\" , \"to\", \"from\", \"at\", \"in\", \"with\", \"within\", \"injured\", \"advise\", \"send\", \n",
    "\"vehicle\",\"received\", \"insured\", \"insure\", \"bodily\", \"liability\", \"adjuster\", \"loss\", \"info\", \"left\", \"right\", \"email\", \"phone\", \"call\", \"time\",\n",
    "'ccdoclink','injury', 'james','mary','john','patricia','robert', 'jennifer','michael','linda','william','elizabeth',\n",
    "'david','barbara','richard','susan','joseph','jessica','thomas','sarah','charles','margaret','christopher','karen',\n",
    "'daniel','nancy','matthew','lisa','anthony','betty','donald','dorothy','mark','sandra','paul','ashley','steven',\n",
    "'kimberly','andrew','donna','kenneth','emily','george','carol','joshua','michelle','kevin','amanda','brian',\n",
    "'melissa','edward','deborah','ronald','stephanie','timothy','rebecca','jason','laura','jeffrey','helen','ryan',\n",
    "'sharon','jacob','cynthia','gary','kathleen','nicholas','amy','eric','shirley','stephen','angela','jonathan',\n",
    "'anna','larry','ruth','justin','brenda','scott','pamela','brandon','nicole','frank','katherine','benjamin',\n",
    "'samantha','gregory','christine','raymond','catherine','samuel','virginia','patrick','debra','alexander','rachel',\n",
    "'jack','janet','dennis','emma','jerry','carolyn','tyler','maria','aaron','heather','henry','diane','jose','julie',\n",
    "'douglas','joyce','peter','evelyn','adam','joan','nathan','victoria','zachary','kelly','walter','christina','kyle',\n",
    "'lauren','harold','frances','carl','martha','jeremy','judith', 'gerald','cheryl','keith','megan','roger','andrea',\n",
    "'arthur','olivia','terry','ann',';awrence','jean','sean','alice','christian','jacqueline','ethan','hannah','austin',\n",
    "'doris','joe','kathryn','albert','gloria','jesse','teresa','willie','sara','billy','janice','bryan','marie','bruce',\n",
    "'julia','noah','grace','jordan','judy','dylan','theresa','ralph','madison','roy','beverly','alan','denise','wayne',\n",
    "'marilyn','eugene','amber','juan','danielle','gabriel','rose','louis','brittany','russell','diana', 'randy',\n",
    "'abigail','vincent','natalie','philip', 'jane','logan','lori','bobby','alexis','harry','tiffany','johnny','kayla'] \n",
    "\n",
    "#notes['body'] = notes['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "#notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing common words as these words might be too vague or general (top 12 most common words)\n",
    "common_words = pd.Series(' '.join(notes['body']).split()).value_counts()[:12]\n",
    "common_words\n",
    "\n",
    "## Ask if we need to increase number of common words to remove more or less "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing common words as these words might be too vague or general (top 12 most common words) (cont.)\n",
    "common_words = list(common_words.index)\n",
    "notes['body'] = notes['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rare words as these words will not offer any info (top 10000 least used words)\n",
    "rare_words = pd.Series(' '.join(notes['body']).split()).value_counts()[-1000:]\n",
    "rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rare words as these words will not offer any info (top 50000ish least used words) (cont.)\n",
    "rare_words = list(rare_words.index)\n",
    "notes['body'] = notes['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_words))\n",
    "notes['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing names as these words are not helpful in our analysis (could add more)\n",
    "names_list = ['macmeekin', 'brum', 'eliza', 'greg', 'stein', 'howard', 'emeka', 'ellison', \n",
    "              'rick', 'gayle', 'slater', 'feeny']\n",
    "\n",
    "notes['body'] = notes['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with nan in the body column\n",
    "notes_subset1['body'] = notes_subset1['body'][notes_subset1['body'] != 'nan']\n",
    "notes_subset2['body'] = notes_subset2['body'][notes_subset2['body'] != 'nan']\n",
    "notes_subset3['body'] = notes_subset3['body'][notes_subset3['body'] != 'nan']\n",
    "notes_subset4['body'] = notes_subset4['body'][notes_subset4['body'] != 'nan']\n",
    "notes_subset5['body'] = notes_subset5['body'][notes_subset5['body'] != 'nan']\n",
    "notes_subset6['body'] = notes_subset6['body'][notes_subset6['body'] != 'nan']\n",
    "notes_subset7['body'] = notes_subset7['body'][notes_subset7['body'] != 'nan']\n",
    "notes_subset8['body'] = notes_subset8['body'][notes_subset8['body'] != 'nan']\n",
    "notes_subset9['body'] = notes_subset9['body'][notes_subset9['body'] != 'nan']\n",
    "notes_subset10['body'] = notes_subset10['body'][notes_subset10['body'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B/c running the code for this section can be time consuming, I decided to import to csv \n",
    "notes.to_csv('notes_processingV3_names.csv', encoding='utf-8', index=False)\n",
    "\n",
    "#notes = pd.read_csv('notes_processingV2.csv')\n",
    "#notes['body] = notes['body].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Document Matrix/Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(notes['body'])\n",
    "tdm = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on Term Document Matrix (not focusing on weights/importance)\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        max_font_size=40,\n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(notes['body'], title = 'Term Document Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "                       stop_words= 'english',ngram_range=(1,2))\n",
    "\n",
    "notes_tfidf = tfidf.fit_transform(notes['body'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [(word, notes_tfidf.getcol(idx).sum()) for word, idx in tfidf.vocabulary_.items()]\n",
    "w = WordCloud(width=800,height=600,mode='RGBA',background_color='white',max_words=2000).fit_words(freqs)\n",
    "plt.imshow(w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx + 1))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "no_features = 1000\n",
    "no_topics = 5\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [notes_subset1['body'], notes_subset2['body'], notes_subset3['body'],\n",
    "          notes_subset4['body'], notes_subset5['body'], notes_subset6['body'], \n",
    "          notes_subset7['body'], notes_subset8['body'], notes_subset9['body'], \n",
    "         notes_subset10['body']]\n",
    "\n",
    "param = [10, 15, 10, 15, 20, 10, 15, 20, 15, 15]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # TF for LDA\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10)\n",
    "    tf = tf_vectorizer.fit_transform(subset[i])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA \n",
    "    lda = LatentDirichletAllocation(n_components = 10, max_iter = 10, learning_method='online', \n",
    "                                    batch_size = 1000, learning_offset = param[i], learning_decay = 0.7).fit(tf)\n",
    "\n",
    "    results_file = \"results_LDA/results_{}_LDA.csv\".format(i)\n",
    "    with open(results_file, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for t in range(10):\n",
    "            writer.writerow([\"Topic {}, {}\".format(t+1, \n",
    "                                ', '.join([tf_feature_names[i] for i in lda.components_.argsort()[t][:-10-1:-1]]))])   \n",
    "for i in range(10):\n",
    "    \n",
    "    # TFIDF for NMF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=10)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(subset[i])\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # NMF algorithm\n",
    "    nmf = NMF(n_components = 10, alpha=.01, init='nndsvd').fit(tfidf)\n",
    "\n",
    "    results_file = \"results_NMF/results_{}_NMF.csv\".format(i+1)\n",
    "    with open(results_file, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for t in range(10):\n",
    "            writer.writerow([\"Topic {}, {}\".format(t+1, \n",
    "                                ', '.join([tfidf_feature_names[i] for i in nmf.components_.argsort()[t][:-10-1:-1]]))])\n",
    "            \n",
    "for i in range(10):\n",
    "    \n",
    "    # TF for LDA (using n-grams)\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10, input = 'content', analyzer = 'word',\n",
    "                                   ngram_range=[1,2], max_features = 5000)\n",
    "    tf = tf_vectorizer.fit_transform(subset[i])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA \n",
    "    lda = LatentDirichletAllocation(n_components = 10, max_iter = 10, learning_method='online', \n",
    "                                    batch_size = 1000, learning_decay = 0.7, learning_offset = param[i]).fit(tf)\n",
    "\n",
    "    results_file = \"results_LDAgram/results_{}_LDAgram.csv\".format(i+1)\n",
    "    with open(results_file, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for t in range(10):\n",
    "            writer.writerow([\"Topic {}, {}\".format(t+1, \n",
    "                                ', '.join([tf_feature_names[i] for i in lda.components_.argsort()[t][:-10-1:-1]]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-3475b5234c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run LDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     lda = LatentDirichletAllocation(n_components = 10, max_iter = 10, learning_method='online', \n\u001b[0;32m---> 17\u001b[0;31m                                     batch_size = 1000, learning_offset = param[i], learning_decay = 0.7).fit(tf)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mresults_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results_LDA/results_{}_LDA.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0midx_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                         self._em_step(X[idx_slice, :], total_samples=n_samples,\n\u001b[0;32m--> 552\u001b[0;31m                                       batch_update=False, parallel=parallel)\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# E-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         _, suff_stats = self._e_step(X, cal_sstats=True, random_init=True,\n\u001b[0;32m--> 433\u001b[0;31m                                      parallel=parallel)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# M-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    384\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_change_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                                               random_state)\n\u001b[0;32m--> 386\u001b[0;31m             for idx_slice in gen_even_slices(X.shape[0], n_jobs))\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# merge result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_iters, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             _dirichlet_expectation_1d(doc_topic_d, doc_topic_prior,\n\u001b[0;32m--> 122\u001b[0;31m                                       exp_doc_topic_d)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmean_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmean_change_tol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subset = [notes_subset1['body'], notes_subset2['body'], notes_subset3['body'],\n",
    "          notes_subset4['body'], notes_subset5['body'], notes_subset6['body'], \n",
    "          notes_subset7['body'], notes_subset8['body'], notes_subset9['body'], \n",
    "         notes_subset10['body']]\n",
    "\n",
    "param = [10, 15, 10, 15, 20, 10, 15, 20, 15, 15]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # TF for LDA\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10)\n",
    "    tf = tf_vectorizer.fit_transform(subset[i])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA \n",
    "    lda = LatentDirichletAllocation(n_components = 10, max_iter = 10, learning_method='online', \n",
    "                                    batch_size = 1000, learning_offset = param[i], learning_decay = 0.7).fit(tf)\n",
    "\n",
    "    results_file = \"results_LDA/results_{}_LDA.csv\".format(i)\n",
    "    with open(results_file, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for t in range(10):\n",
    "            writer.writerow([\"Topic {}, {}\".format(t+1, \n",
    "                                ', '.join([tf_feature_names[i] for i in lda.components_.argsort()[t][:-10-1:-1]]))])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10], \n",
    "                 'learning_decay': [0.9, 0.7], \n",
    "                 'learning_offset': [10, 15, 20], \n",
    "                 'batch_size': [1000]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(learning_method = 'online')\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10)\n",
    "data_vectorized = tf_vectorizer.fit_transform(notes_subset10['body'])\n",
    "model.fit(data_vectorized)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()\n",
    "\n",
    "notes_subset1 = pd.read_csv('notes_subset/notes_subset1_v2.csv')\n",
    "notes_subset1['body'] = notes_subset1['body'].astype(str)\n",
    "notes_subset1 = notes_subset1[notes_subset1['body'] != 'nan']\n",
    "notes_subset1['body'] = notes_subset1['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "notes_subset1['body'] = notes_subset1['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "\n",
    "notes_subset2 = pd.read_csv('notes_subset/notes_subset2_v2.csv')\n",
    "notes_subset2['body'] = notes_subset2['body'].astype(str)\n",
    "notes_subset2 = notes_subset2[notes_subset2['body'] != 'nan']\n",
    "notes_subset2['body'] = notes_subset2['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset2['body'] = notes_subset2['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset3 = pd.read_csv('notes_subset/notes_subset3_v2.csv')\n",
    "notes_subset3['body'] = notes_subset3['body'].astype(str)\n",
    "notes_subset3 = notes_subset3[notes_subset3['body'] != 'nan']\n",
    "notes_subset3['body'] = notes_subset3['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset3['body'] = notes_subset3['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset4 = pd.read_csv('notes_subset/notes_subset4_v2.csv')\n",
    "notes_subset4['body'] = notes_subset4['body'].astype(str)\n",
    "notes_subset4 = notes_subset4[notes_subset4['body'] != 'nan']\n",
    "notes_subset4['body'] = notes_subset4['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset4['body'] = notes_subset4['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset5 = pd.read_csv('notes_subset/notes_subset5_v2.csv')\n",
    "notes_subset5['body'] = notes_subset5['body'].astype(str)\n",
    "notes_subset5 = notes_subset5[notes_subset5['body'] != 'nan']\n",
    "notes_subset5['body'] = notes_subset5['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset5['body'] = notes_subset5['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset6 = pd.read_csv('notes_subset/notes_subset6_v2.csv')\n",
    "notes_subset6['body'] = notes_subset6['body'].astype(str)\n",
    "notes_subset6 = notes_subset6[notes_subset6['body'] != 'nan']\n",
    "notes_subset6['body'] = notes_subset6['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset6['body'] = notes_subset6['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset7 = pd.read_csv('notes_subset/notes_subset7_v2.csv')\n",
    "notes_subset7['body'] = notes_subset7['body'].astype(str)\n",
    "notes_subset7 = notes_subset7[notes_subset7['body'] != 'nan']\n",
    "notes_subset7['body'] = notes_subset7['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset7['body'] = notes_subset7['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset8 = pd.read_csv('notes_subset/notes_subset8_v2.csv')\n",
    "notes_subset8['body'] = notes_subset8['body'].astype(str)\n",
    "notes_subset8 = notes_subset8[notes_subset8['body'] != 'nan']\n",
    "notes_subset8['body'] = notes_subset8['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset8['body'] = notes_subset8['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset9 = pd.read_csv('notes_subset/notes_subset9_v2.csv')\n",
    "notes_subset9['body'] = notes_subset9['body'].astype(str)\n",
    "notes_subset9 = notes_subset9[notes_subset9['body'] != 'nan']\n",
    "notes_subset9['body'] = notes_subset9['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset9['body'] = notes_subset9['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "notes_subset10 = pd.read_csv('notes_subset/notes_subset10_v2.csv')\n",
    "notes_subset10['body'] = notes_subset10['body'].astype(str)\n",
    "notes_subset10 = notes_subset10[notes_subset10['body'] != 'nan']\n",
    "notes_subset10['body'] = notes_subset10['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in CSAA_stop_words))\n",
    "notes_subset10['body'] = notes_subset10['body'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_subset1 = pd.read_csv('notes_subset/notes_subset1.csv')\n",
    "notes_subset1['body'] = notes_subset1['body'].astype(str)\n",
    "notes_subset1 = notes_subset1[notes_subset1['body'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# TF for LDA\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10)\n",
    "tf = tf_vectorizer.fit_transform(notes_subset1['body'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 10, max_iter = 10, learning_method='online', learning_decay = 0.7,\n",
    "                                learning_offset = 20, batch_size = 1000)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "prepare = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8888/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Feb/2019 16:20:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Feb/2019 16:20:17] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Feb/2019 16:20:18] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Feb/2019 16:20:18] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.show(prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
