{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarity\n",
    "\n",
    "In the last couple weeks we looked at different kinds of supervised learning models using scikit.  Now we'll turn to unsupervised models.  The ones we'll look at fall into three main categories:\n",
    "\n",
    "1. Dimensionality reduction: taking a dataset with a large number of features and \"compressing\" it to a smaller number of features.\n",
    "2. Neighbor-finding: taking a specific data point and finding the other data points most similar to it.\n",
    "3. Clustering: finding \"natural\" groupings of data points based on their features.\n",
    "\n",
    "We'll also look at topic modeling, which is sort of about finding \"what is shared\" in documents rather than finding groups of documents that \"share something\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exploration, we'll use a small dataset consisting of the top 100 most-viewed Wikipedia articles over a certain period (around early 2017).  First we'll load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# this is just to let us see a bit more of the article text\n",
    "pandas.options.display.max_colwidth = 100\n",
    "\n",
    "df = pandas.read_csv('../Data/wikipedia_top_100.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to do a train/test split, because this is unsupervised learning, which means there is no test!  The agony and the ecstasy of unsupervised learning is that there is no \"gold standard\" of results to use to check our models.  In other words, we never really know whether we're right or not.\n",
    "\n",
    "As with the supervised methods we used before, our unsupervised methods will typically be working with a bag-of-words vectorization.  So let's set that up similar to what we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "\n",
    "vect = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.8, max_features=2000, sublinear_tf=True)\n",
    "vect.fit(df.Text)\n",
    "features = vect.transform(df.Text)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a few variations here:\n",
    "\n",
    "* Instead of using stop words, I set `max_df` to 0.8.  `max_df` sets the maximum document frequency; a fractional value like 0.8 is interpreted as a proportion, so these means \"exclude any words that occur in more than 80% of documents\".  This can sometimes be used instead of stop words, although the threshold may need some tweaking depending on your use case.  (Using an integer instead would set an absolute number of documents as the limit; for instance `max_df=100` would exclude all words that occurred in more than 100 documents.  Usually a proportion is more appropriate for `max_df`.)\n",
    "* I used `max_features` to cap the number of features in the resulting matrix at 2000.  I did this because some of the techniques we'll be using don't deal well with the gargantuan numbers of features we can get from an unrestricted vectorization.\n",
    "* Finally, I used `sublinear_tf`.  This logarithmically scales the term frequencies, so that if a word occurs, say, 20 times, its frequency will instead be counted as log(1+20).  In other words, more frequent words have their frequencies reduced relative to less frequent words.  I used this here because some of the techniques we'll be using perform better with this modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction (sometimes also called \"decomposition\") means reducing the number of features that are used to represent each data point.  We'll mostly be using this as a way to visualize datasets, but it can also be used as a processing step in a machine learning pipeline.  Some algorithms become very slow if the data has a large number of features (i.e., a large number of columns), and in many cases, when there are a large number of columns, many of them carry relatively little information.  In such situations, dimensionality reduction can be used to compress the data to a smaller set of features that can be processed more quickly, while still retaining most of the information from the original.\n",
    "\n",
    "But, like I said, we'll use dimensionality reduction for visualization.  In that context, we usually want to reduce the number of dimensions all the way to two, because that's what we're able to display in a plot.  (As we saw in the plotting tour, we could sometimes use color for a third dimension, but here we'll reserve color for showing clusters, in the later part of this notebook.)\n",
    "\n",
    "The [scikit documentatio](https://scikit-learn.org/stable/modules/decomposition.html) shows the kinds of dimensionality reduction techniques available to us.  Two of them --- Latent Dirichlet Allocation and Nonnegative Matrix Factorization --- can be used for topic modeling, which we'll look at later.  For now, we'll be using Truncated Singular Value Decomposition because it has one major advantage: it can be used with sparse data.  Since we're mostly working with sparse data, that's important for us.  For this small dataset, we'll sometimes be able to get away with \"densifying\" our sparse data, but with large datasets that's often not feasible, so we'll work with TruncatedSVD since it's more scalable.\n",
    "\n",
    "TruncatedSVD is in the `sklearn.decomposition` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a TruncatedSVD model, we need to tell it how many dimensions (or \"components\") we want to reduce to.  Since we're using this for plotting, we'll reduce to two dimensions.  Then we fit the model on our text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = sklearn.decomposition.TruncatedSVD(n_components=2)\n",
    "svd.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TruncatedSVD is in someways akin to how we used our vectorizer: once we've fit it, we can use the `.transform` method to get new features out of it.  The difference is that, whereas our vectorizers take a single column (of text documents) and create a large number of features (one for each word, roughly), TruncatedSVD takes in a bunch of columns (those word features) and gives us back just two features.  We can see what the result looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.transform(features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still has 100 rows, one for each of our documents, but now it only has two columns.  We'll stick those into our DataFrame as new columns called \"SVDX\" and \"SVDY\".  Since the result of the transform is a numpy array, we have to access each column by its numerical index.  (Well, we don't *have* to, there are other ways to do this, but this is the simplest one to explain here.)  Remember that `:` means \"all\" when indexing, and the order of indexes is \"rows, columns\", so `[:, 0]` means \"all rows, column 0\" and `[:, 1]` means \"all rows, column 1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_features = svd.transform(features)\n",
    "df[\"SVDX\"] = svd_features[:, 0]\n",
    "df[\"SVDY\"] = svd_features[:, 1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about those names: I called them \"X\" and \"Y\", but they're actually totally arbitrary.  This is an important point for dimensionality reduction techniques in general: they reduce the number dimensions, but the new dimensions they give you have no particular \"meaning\".  The results of TruncatedSVD are just arbitrary numbers (although in some cases you might be able to come up with an interpretation of what each dimension \"means\" by pondering the results).  Also, running TruncatedSVD multiple times could result in slightly different results; usually they will be basically the same, but you might find that the coordinates are flipped or rotated.\n",
    "\n",
    "Anyway, the cool thing about this is that now we've compressed each text down to just two numbers, which we can use as X and Y coordinates to make a scatter plot where each point represents one Wikipedia article.  Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import mplcursors\n",
    "\n",
    "ax = df.plot.scatter(x=\"SVDX\", y=\"SVDY\")\n",
    "cursor = mplcursors.cursor(ax)\n",
    "cursor.connect('add', lambda sel: sel.annotation.set_text(df.Title.iloc[sel.target.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty rad.  You can have some fun clicking around and seeing what each point is.  It's quite remarkable how points that are closer to each other tend to be related, even though we've collapsed 2000 dimensions into just two.  (Of course, there are some oddballs in there.)\n",
    "\n",
    "Since it may be that not everyone was able to install `mplcursors`, I'm going to show an alternative way to do the plot, where the text of each article title is stuck right in there instead of appearing on click.  This is not so nice, since it clutters up the plot, but it's better than nothing.  Basically we iterate over the data, and use the `pyplot.text` function that we saw before to plot each bit of text.  I chopped the titles off to only the first 15 characters, to reduce clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"SVDX\", y=\"SVDY\")\n",
    "for ix, row in df.iterrows():\n",
    "    pyplot.text(row[\"SVDX\"], row[\"SVDY\"], row[\"Title\"][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Before we move on to neighbors and clustering, we have to talk about the basis on which we'll be doing it.  If you look at [the scikit documentation](https://scikit-learn.org/stable/modules/clustering.html) about clustering algorithms, one thing you'll notice is that some of them accept an argument called \"affinity\".  This refers to the method used to calculate \"distances\" between points (that is, documents, in our case).  \"Affinity\" is used rather than distance to indicate that we are measuring similarity rather than distance.\n",
    "\n",
    "The most common distance metric in everyday life is the Euclidean metric, which is essentially the Pythagorean theorem.  But this metric has some undesirable properties when dealing with text data.  A useful alternative is the \"cosine similarity\", which computes the angle between two vectors.  (This angle would be the \"affinity\", because smaller angles mean the vectors point in nearly the same direction; the corresponding distance metric would be $1 - \\cos \\textbf v$.)  The other metric provided by scikit is the \"Manhattan\" or \"city clock\" distance.  In some situations, scikit can also make use of metric from the underlying `scipy` library, which includes many more.\n",
    "\n",
    "One simple way to understand what the metrics are doing is to just compute the pairwise distances on a set of articles.  Let's use these few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df[df.Title.isin([\n",
    "    \"Bill Clinton\", \"Ronald Reagan\", \"Hillary Clinton\", \"Earth\", \"Split (2016 American film)\", \"Fantastic Beasts and Where to Find Them (film)\"\n",
    "])]\n",
    "samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit provides a `pairwise_distances` function that will compute a table showing the distances between all pairs of points in our set.  To use it, we need to get the corresponding rows from `features` and pass them along.  Here we'll use the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise_distances(features[samp.index, :], metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, it's more convenient to make this into a DataFrame.  The row and column labels here will be the same, since each cell is showing the distance between a pair of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_dists = pandas.DataFrame(sklearn.metrics.pairwise_distances(features[samp.index, :], metric='cosine'), index=samp.Title, columns=samp.Title)\n",
    "samp_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to what we saw with Spacy word vectors, but now instead of computing similarities between words, we're computing similarities (or rather, differences) between entire documents.  As we saw before, Spacy could do this too, but the way we're doing it now is quite different.  With Spacy, when you compute the similarity between two documents, it's computing a similarity between their vectors, and those vectors are based on averaging the vectors of all vectors in the document.  The individual components in those word vectors had no definable meaning.  Here, the individual components *do* have definable meaning: each component represents the frequency (with IDF weighting) of a particular word in the corpus.  Thus, the distances here are not based on averaging across all words, but on considering the frequencies of all words simultaneously.  Documents with similar word-frequency profiles will show up as similar.\n",
    "\n",
    "So, as we can see (and as we would expect), Bill Clinton is very similar to Hillary Clinton, and both are similar to Ronald Reagan, whereas all three of them are rather different from Earth or the two movies in the dataset.  The two movies are more similar to each other than they are to the other articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors\n",
    "\n",
    "Now that we understand what's up with the metrics, we can try to use them to get the documents most similar to a given one.  To do this, we'll use a scikit model called (shockingly) `NearestNeighbors`, which is (again shockingly) in the `sklearn.neighbors` module.\n",
    "\n",
    "When creating the model, we pass the distance metric we'd like to use.  Here we'll use the cosine metric.  Then we fit the model on our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "nn = sklearn.neighbors.NearestNeighbors(metric='cosine')\n",
    "nn.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, actually getting the neighbors for a particular article is not as straightforward as we might hope.  The problem is that we need to use `features` to look up the neighbors, but all the human-readable information like the article text is in `df`, not `features`.  We could tack the features on to the DataFrame, like we did with SVDX and SVDY, but that would get unwieldy; it's one thing to expand our DataFrame with two columns for plotting, but pasting in 2000 extra columns is another matter.  Also, that would require densifying our data, which we'd like to avoid if possible.\n",
    "\n",
    "So what we have do is find the article we want, and note its position in the DataFrame.  In our case this is given by its index.  Suppose we want to find the articles similar to \"Earth\".  Here's our \"Earth\" article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Title == \"Earth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its index is 1.  That means that row #1 of the features table holds its data.  We get that row (with all columns) and pass it to the `kneighbors` method of our model, along with the number of neighbors we want to find (five for the moment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(features[1, :], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back two somewhat mysterious arrays of numbers.  The documentation can help us interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.kneighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation clarifies that the function returns two items: `dist`, which is the array of distances (how close the neighbors are to our target point), and `ind`, which is the array of indices (basically the row numbers of the neighbors in our `features` array).  To look the neighbors up in `df` again, the indices are what we want.  The indices array is the second element of the tuple returned by `kneighbors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(features[1, :], 5)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by the double brackets that it's actually a two-dimensional array.  This may seem annoying in this case, but it'd be handy if we wanted to find the neighbors of multiple points at once.   To use the indices, we need to grab that first row from the indices array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(features[1, :], 5)[1][0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use that to index into our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[nn.kneighbors(features[1, :], 5)[1][0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the article most similar to Earth is. . . Earth.  Well, I suppose that makes sense, but usually it won't be very useful to us.  In many cases you'll want to get one more neighbor than you really need, and then drop the first column of the indices array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[nn.kneighbors(features[1, :], 6)[1][0, 1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Remember that `1:` there, in the columns position, means to take all columns starting from #1, which is the second column, since they're numbered starting at zero.)\n",
    "\n",
    "So now we can see the articles most similar to our chosen article.  It makes a kind of sense, because these are all \"large-scale\" things.  These similarities also probably reflect certain conventions about how Wikipedia articles are structured.  For instance, the articles on countries, like the article on Earth, will devote a significant amount of space to geographical features.\n",
    "\n",
    "Let's try another, quite different one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Title == \"LeBron James\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get LeBron's neighbors the same way, now using the row #24 of `features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[nn.kneighbors(features[24, :], 6)[1][0, 1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite reasonable!\n",
    "\n",
    "One nice thing about this process is that it's actually quite fast, even on fairly large datasets.  Here I'll load in the much larger Wikipedia sample we used for the summarization task, quickly create another vectorizer for it, and fit a new neighbors model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_wiki = pandas.read_csv('../Data/wikipedia_sample.csv')\n",
    "\n",
    "big_vect = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.8, min_df=5, sublinear_tf=True)\n",
    "big_vect.fit(big_wiki.Text)\n",
    "big_feat = big_vect.transform(big_wiki.Text)\n",
    "\n",
    "big_nn = sklearn.neighbors.NearestNeighbors(metric='cosine')\n",
    "big_nn.fit(big_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here I didn't limit the model to only 2000 features, so it has a very large sparse feature matrix.\n",
    "\n",
    "(I also used `min_df`, which sets a *minimum* document frequency.  Here I used an integer value, meaning that words that occur in fewer than 5 articles will be excluded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find some appealing article to use as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_wiki[big_wiki.Title==\"Thresher shark\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_wiki.iloc[big_nn.kneighbors(big_feat[1477, :], 6)[1][0, 1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those seem like good related articles.  (Remember that this dataset is only a random sample of Wikipedia articles, so of course there are many more closely related articles that don't happen to be in this sample.)  Also, notice that, although the model took a little while to fit, once it is fit, obtaining the neighbors is essentially instantaneous.  This is useful for knowledge-retrieval systems with a database that doesn't change very often; once you have a nearest-neighbor model set up, using it to get related content can be very fast.\n",
    "\n",
    "Just for fun, let's show how we could roll that into a function.  All we have to do is grab the index from the one-row DataFrame we get from our Title search, and use that in the `kneighbors` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(title):\n",
    "    row_num = big_wiki[big_wiki.Title == title].index\n",
    "    return big_wiki.iloc[big_nn.kneighbors(big_feat[row_num, :], 6)[1][0, 1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just call our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_articles(\"Sports in Houston\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.sample()` to get a random smattering of articles, pick one we like, and do this a few times.  This space is for you to goof around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_wiki.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_articles(\"Biomechatronics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, remember that we don't have to restrict ourselves to articles in the original dataset.  We could use our vectorizer to vectorize any text we like, and use that as the basis for the neighbor search.  For instance, suppose I want to know about robots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feat = big_vect.transform([\"I want to know about robots\"])\n",
    "big_wiki.iloc[big_nn.kneighbors(query_feat, 5)[1][0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line above should look familiar from our sentiment analysis task a couple weeks ago: I've used the vectorizer to get the word features for my mini-text (wrapping it in a list because `transform` expects a sequence of documents, not just a single document).  Then I passed that to `kneighbors` instead of using `big_feat`.  As you may have guessed, I'm trying to get you to realize how you could start to build fun and useful tools by taking the machine-learning models we've developed and wrapping them into simple functions.  Like how about this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wiki(query):\n",
    "    query_feat = big_vect.transform([query])\n",
    "    return big_wiki.iloc[big_nn.kneighbors(query_feat, 5)[1][0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_wiki('fine art')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you start to see the potential of this kind of thing.  Now we'll move on to clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering (or \"cluster analysis\") involves taking a bunch of data points and trying to figure out if we can uncover implicit (or \"latent\" categories) based on which points are similar to which other points.  Unlike the classification problems we did before, this is unsupervised learning because we don't know ahead of time what the categories are; we're hoping the model can find them for us.\n",
    "\n",
    "There are a wide variety of different algorithms for clustering, and scikit gives us access to many of them, as can be seen in [the documentation](https://scikit-learn.org/stable/modules/clustering.html).  The main distinction to be aware of is that about half of them require us to specify the number of clusters up front, while the other half don't.  (The ones that don't require it typically allow specification of some other sort of parameter that influences the creation of the clusters.)\n",
    "\n",
    "The only method that can make use of sparse feature matrices is K-Means, so we'll start with that.  We don't have to specify a metric, since K-Means only works with the Euclidean metric, but it does require us to specify `n_clusters`, the number of clusters we want.  Let's start with 8, because that's my favorite number.  Given that, creating and fitting the model is old hat by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=8)\n",
    "kmeans.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the supervised-learning models we saw before, we can call `.predict` on this.  However the nature of the prediction is a bit different.  What the model \"predicts\" is just a number which is the \"label\" for a particular cluster.  Also, since this unsupervised learning, we just pass the same `features` back to `.predict`.  (The same information is also available in the `.labels_` attribute of the fitted model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers have no inherent meaning; it's not like items in cluster 3 are \"better\" or \"bigger\" than items in cluster 2.  All we know or care about is that items with the same number are in the same cluster (as best as the algorithm can determine).  Also, importantly, this algorithm is *non-deterministic*: running it multiple times may give different results.  Or sometimes it may give results that *look* different but actually aren't, because the cluster labels may be permuted (so that the clusters still contain the same items, but they've just been given different numbers than before).\n",
    "\n",
    "The numbers are in the order that the data points were passed in, so the first number there corresponds to the cluster for our first data point.  We can stick this information into our DataFrame as a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = kmeans.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to embellish the kind of plot that we did earlier.  We'll do the same plot, but now we'll tell it to use the new `Cluster` column for colors.  Since we have 8 clusters, we'll generate an 8-color colormap using `pyplot.get_cmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import mplcursors\n",
    "\n",
    "ax = df.plot.scatter(x=\"SVDX\", y=\"SVDY\", c=\"Cluster\", colormap=pyplot.get_cmap(\"jet\", 8))\n",
    "cursor = mplcursors.cursor(ax)\n",
    "cursor.connect('add', lambda sel: sel.annotation.set_text(df.Title.iloc[sel.target.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the clusters don't necessarily align with the positions of the points in our SVD-transformed space.  That is generally to be expected, especially for high-dimensional datasets like this: SVD forces the data to fit into two dimensions, whereas the clustering makes use of all the dimensions in the original data to determine similarity.\n",
    "\n",
    "If we click around, those clusters look fairly reasonable.  But are they?  How would we find out?\n",
    "\n",
    "Well, that's a really tough question.  There are many ways to assess the \"goodness\" of clusters, but none of them is ironclad, because we don't really know how many clusters there are supposed to be, so we have no way to really check our answers.\n",
    "\n",
    "Faced with this uncertainty, we can again turn to some metrics, but this time a different kind of metrics.  In supervised learning, we used classification and regression metrics to assess how close our predictions were to reality.  Here we don't know the \"reality\" of how many clusters there ought to be.  But one intuitive notion we have is that documents that are in the same cluster ought to be more similar to one another than they are to documents in some other cluster.  There are various clustering metrics that try to get at this notion, and they differ in exactly how they operationalize it.\n",
    "\n",
    "The metric that we'll use for this class is a common one called the \"silhouette score\".  Roughly speaking, what this does is it computes the average distance from every point to every other point, and averages this distance for each cluster to get the average distance from each point to each cluster.  Then it compares the average distance from each point to its own cluster with the average distance from that point to the \"next best\" cluster (i.e., the cluster with the smallest average distance, besides the point's own cluster).  It compares these to get a score for each point, and then averages all the scores for all the points to get an overall metric for the goodness of the whole clustering.\n",
    "\n",
    "This metric is computationally intensive to compute, since it requires computing the distances between all pairs of points.  This means it is often not usable for large datasets.  However, it's the method we'll use here, since it is one of the easiest to interpret: it's a number between -1 and 1, where higher numbers mean better clustering.  (Basically, higher numbers mean that each point is much closer, on average, to other points in its own cluster than it is to points in other clusters --- in other words, the clusters are well separated.)\n",
    "\n",
    "To compute the silhouette score, we use the aptly named `silhouette_score` function from `sklearn.metrics`.  We pass it the matrix of features from which the clusters were calculated, along with the cluster labels our model gave us.  We also pass it a metric to use.  Here we'll use Euclidean, since that's what the K-Means algorithm was using.  (It can also be illuminating to use the cosine metric here, to see how well the Euclidean-derived clusters found by K-Means hold up when compared using cosine distances.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "sklearn.metrics.silhouette_score(features, df.Cluster, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look super great, but it's all relative.  What kind of score you can expect depends on the data and how naturally it falls into clusters.  Some data sets are inherently noisy and it's hard to find clearly separated clusters in them.\n",
    "\n",
    "If we really want to be thorough, one thing we can do is loop over some range of cluster numbers, redo the cluster analysis for each number, and compute the silhouette score for each possibility.  This can be somewhat time-consuming, but it's better than just guessing.  Here's how we might do this for K-Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clust in range(2, 10):\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=n_clust)\n",
    "    kmeans.fit(features)\n",
    "    score = sklearn.metrics.silhouette_score(features, kmeans.labels_, metric='euclidean')\n",
    "    print(\"With {} clusters, the silhouette score is {:.3f}\".format(n_clust, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Don't be too alarmed by that `{:.3f}` thing.  That's a way of getting the code to display only three decimal places.  Interested parties can read about the joys of format strings in [the documentation](https://docs.python.org/3/library/string.html#formatstrings).  Others can simply smile and nod.)\n",
    "\n",
    "Ideally, we'd look at that output and pick the number of clusters with the highest silhouette score.  Ideally.  Unfortunately, reality is different, because, as mentioned, K-means is not deterministic, so if we rerun that, we might get different scores.  Remember what I said about the agony and the ecstasy of unsupervised learning.  Yeah, this is that.\n",
    "\n",
    "Anyway, it's looking like maybe 6 or 7 is the \"right\" number of clusters according to the silhouette score.  We'll go with 6.  So I'll re-run the clustering with 6 clusters, and stick the results into the `Cluster` column of our DataFrame (overwriting the values that were already there from before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters=6)\n",
    "kmeans.fit(features)\n",
    "df['Cluster'] = kmeans.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our plot again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import mplcursors\n",
    "\n",
    "ax = df.plot.scatter(x=\"SVDX\", y=\"SVDY\", c=\"Cluster\", colormap=pyplot.get_cmap(\"jet\", 6))\n",
    "cursor = mplcursors.cursor(ax)\n",
    "cursor.connect('add', lambda sel: sel.annotation.set_text(df.Title.iloc[sel.target.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, maybe the clusters don't look *that* much better, but at least now we have some firmer foundation for relying on them.\n",
    "\n",
    "Once we've got some number of clusters, a common task involves pondering them and trying to come up with some meaningful description of each one.  Despite the low silhouette score, the clusters here actually seem not bad: there's one consisting of movies and TV shows, one consisting of musicians (with a couple actors), one consisting of atheletes, a small one consisting of TV channels, etc.  There's one big cluster whose nature is a bit murkier, though.  (Take all these descriptions with a grain of salt!  Remember that the clusters may change on each run of the code, so when you run this and look at it, you may see slightly different results than what are described here.  However, overall it's likely that the broad outlines of the category \"meanings\" will remain similar.)\n",
    "\n",
    "In the assignment, you'll explore other clustering algorithms.  They'll require you to densify your feature array using `.toarray()`.  For instance, I could do that here like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in an earlier class, you have to be careful when doing this, because if the feature matrix is large, converting to dense can suddenly gobble up a lot of memory.  In this case, our DataFrame only has 100 rows, so it's okay.  (You'll be densifying features for a similarly small set in the assignment.)  This is also one reason I set `max_features=2000` when creating the vectorizer; limiting the number of features keeps the size down if we have to densify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "For the last part of this notebook, we'll take a brief look at topic modeling.  Topic modeling is similar to clustering in that it involves grouping.  However, it is a bit different in that it typically involves two-way grouping.  The idea is that each document is considered as a mixture of different topics, and each topic is a mixture of different words.  Each word has a certain degree of association with each topic, and each topic has a certain degree of association with each document.  Thus, topic modeling is somewhat like clustering both the words and the documents: words that occur in similar documents may represent similar topics, and documents that share similar words may be about similar topics.\n",
    "\n",
    "One of the most common statistical models for topic modeling is called Latent Dirichlet Allocation.  As usual, scikit has it ready and waiting for us in `sklearn.decomposition`.  As with many clustering methods, most topic allocation methods require us to specify the number of topics up front.\n",
    "\n",
    "One quirk of Latent Dirichlet Allocation is that it won't work well with Tf-Idf data; it needs the raw counts.  This is because the underlying algorithm takes account of term frequency and document frequency in building the topic model; if we pass it Tf-Idf data, the two kinds of information are already smushed together and LDA can't tease them apart to do its work.  Accordingly, we'll create a CountVectorizer to use for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "count_vect = sklearn.feature_extraction.text.CountVectorizer(max_df=0.8, min_df=2, stop_words=stop_words)\n",
    "count_vect.fit(df.Text)\n",
    "count_feat = count_vect.transform(df.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cleverly slipped in here an example of how to use an alternative stopword list when creating your vectorizer.  NLTK has a \"stopwords\" corpus.  You can access its English stopwords as shown, and pass the list to the vectorizer.  You may find this list better than using scikit's `'english'` stopword list in some cases.\n",
    "\n",
    "Creating the model is similar to all the other models we've created.  (You may also have noticed that the way I describe how to fit each model is surprisingly similar to how I describe fitting all the other models.  Oh scikit, you're my rock.)  Since we found 6 clusters above, 6 topics seems like a decent place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = sklearn.decomposition.LatentDirichletAllocation(n_components=6)\n",
    "lda.fit(count_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model has a `.components_` attribute that is somewhat akin to the `.coef_` attribute we saw for various supervised learning models.  It contains a 2D array in the shape `(n_topics, n_features)`.  In other words, one row per topic, one column per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in this matrix represents the strength of association between a word and a topic.  As usual, we'll convert this to a DataFrame to make the most of it.  We can use the `.T` attribute of this array to transpose it so that the topics are in the columns instead of the rows, and then wrap it in a DataFrame.  As before, we'll call the `.get_feature_names` method of our vectorizer to get the words in the appropriate order, and use this as the index of our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pandas.DataFrame(lda.components_.T, index=count_vect.get_feature_names())\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do as we did before with the coefficients of different models.  If we sort by column 0, we'll see at the top the words most associated with that topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[0].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the cluster labels, the topic numbers are meaningless.  All we know is we've got six different dimensions on which the documents can be characterized.  Typically, we use the top few words in each topic to help us take a stab at describing what that topic is \"about\".  To aid us in that, we can write a loop that shows the top 10 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in topics:\n",
    "    print(\"Topic {} words: {}\".format(col, list(topics[col].sort_values(ascending=False).head(10).index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those topics aren't perfect but we can see some structure there.\n",
    "\n",
    "Assessing the quality of topics is tougher than it is for clusters.  There are methods based around notions of \"coherence\" that, for instance, use word vectors to compute the similarities between the top words in each topic.  The idea is that if the words in a topic are semantically similar, the topic has some amount of coherent meaning as a whole.  We won't get into such methods in this class, however.\n",
    "\n",
    "The topic model allows us to do one more thing: we can use its `.transform` method to get a matrix of the shape `(n_documents, n_topics)`.  Whereas `.components_` lets us see the association between words and topics, `.transform` shows us the association between *documents* and topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.transform(count_feat)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column represents the document's association with that particular topic.  We'll make another DataFrame, this time using the `Title` column of our original data, since the result of `transform` has one row for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = pandas.DataFrame(lda.transform(count_feat), index=df.Title)\n",
    "doc_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the top documents for each topic, instead of the top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in doc_topics:\n",
    "    print(\"Topic {} documents: {}\".format(col, list(doc_topics[col].sort_values(ascending=False).head(10).index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a last blast, let's use the Pandas `idxmax()`method to compute the topic with the highest score for each document.  `.idxmax()` returns the label of the highest scoring row or column for each column or row, depending on what we pass for the `axis` argument.  Here I pass `columns'`, telling it to give me the index of the highest-scoring column in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics.idxmax(axis='columns').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose I come up with a simple name for each column and make a little dictionary of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {\n",
    "    0: \"Sports\",\n",
    "    1: \"International\",\n",
    "    2: \"Entertainment\",\n",
    "    3: \"Leaders\",\n",
    "    4: \"Politics\",\n",
    "    5: \"Media\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use `.map` to map my column indices to their names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics.idxmax(axis='columns').map(topic_names).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, I can stick these back into my original DataFrame.  (I have to use `.values` here because the index on my list of topics is the title, while the index on my original table is just a number.  Pandas likes to match up indexes, and will throw away values that don't match up, so using `.values` tells it to just use the raw values in the order they're in.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TopicNum'] = doc_topics.idxmax(axis='columns').map(topic_names).values\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've developed a little pipeline that takes a bunch of wikipedia articles and assigns them a label based on what their topic is.  If you poke around you'll see not all of them are correct, but overall it's pretty nice.  The only part that a human has to manually do is write the label for each category; the topic model can determine which articles go into which topics, but it can't \"think of\" a word or phrase to describe each one, so we have to do that on our own (at least, at the level we're at here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare\n",
    "\n",
    "A classic application of these techniques is applying them to literary texts to see if similar ones can be grouped together.  A classic among classics is applying these techniques to Shakespeare to see if plays that humans think are similar (e.g., comedies vs. tragedies) will be grouped together.\n",
    "\n",
    "We bow to tradition and load in our Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = pandas.read_csv('../Data/shakespeare-plays/Shakespeare_data.csv')\n",
    "shakespeare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset contains a lot of detail.  It has every play broken down line by line and shows us who said each line, what act and scene it was in, etc.  That's more than we really need, since we're going to just treat each complete play as a single data point.\n",
    "\n",
    "Also notice that some of the lines have \"NaN\" if they're not real lines (such as the scene titles).  Pandas allows us to drop those with `.dropna()`.  (By default it gets rid of all rows that have any NaN values at all.)  Having done that, we can `.map` a function that just joins all the lines together, separated by linebreaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = shakespeare.dropna().groupby('Play').PlayerLine.apply(lambda g: '\\n'.join(g.values))\n",
    "plays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has now become a Series with the play's title as the index.  Since we'll want to add more columns later, we'll wrap it into a DataFrame with these values as the `Text` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = pandas.DataFrame({\"Text\": plays})\n",
    "plays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a vectorizer as before.  Since we only have 36 data points this time, I upped the number of features to 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvect = sklearn.feature_extraction.text.TfidfVectorizer(max_features=5000, max_df=0.8, min_df=2, sublinear_tf=True)\n",
    "ssvect.fit(plays.Text)\n",
    "ssfeat = ssvect.transform(plays.Text)\n",
    "ssfeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can redo the procedure we did above.  We'll make new SVDX and SVDY dimensions from our word features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_svd = sklearn.decomposition.TruncatedSVD(n_components=2)\n",
    "ss_svd.fit(ssfeat)\n",
    "newdim = ss_svd.transform(ssfeat)\n",
    "plays[\"SVDX\"] = newdim[:, 0]\n",
    "plays[\"SVDY\"] = newdim[:, 1]\n",
    "plays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plays.plot.scatter(x=\"SVDX\", y=\"SVDY\")\n",
    "cursor = mplcursors.cursor(ax)\n",
    "cursor.connect('add', lambda sel: sel.annotation.set_text(plays.index[sel.target.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, Shakespeare's plays are divided into three categories: comedies, tragedies, and histories.  The \"histories\" appear to be fairly well separated.  Most of the comedies are in the lower left and the tragedies in the lower right, but the boundary between them is not sharp,\n",
    "\n",
    "Let's see if a cluster analysis can identify them.  Naturally we'll start by looking for three clusters.  This time I'll use a different clustering algorithm called agglomerative (or \"hierarchical\") clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_clust = sklearn.cluster.AgglomerativeClustering(n_clusters=3)\n",
    "ss_clust.fit(ssfeat.toarray())\n",
    "plays['Clust'] = ss_clust.labels_\n",
    "plays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notice that I had to use `.toarray()` to convert my feature matrix to a dense array.  This is okay here because our number of data points is small.)\n",
    "\n",
    "We can do our plot again and color the points by cluster as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plays.plot.scatter(x=\"SVDX\", y=\"SVDY\", c='Clust', colormap=pyplot.get_cmap('jet', 3))\n",
    "cursor = mplcursors.cursor(ax)\n",
    "cursor.connect('add', lambda sel: sel.annotation.set_text(plays.index[sel.target.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories align fairly well with traditional categorizations.  Some of the \"incorrectly\" categorized plays are those which Shakespeare scholars have also considered difficult to categorize.  For instance *Troilus and Cressida* (traditionally considered a tragedy but here grouped with the comedies) and *All's Well That Ends Well* (vice versa) are among those termed \"problem plays\" which defy easy categorization.  Also, the \"late romances\" (*Pericles*, *Cymbeline*, *The Winter's Tale*, and *The Tempest*) are traditionally considered comedies, but are here grouped the tragedies, but some scholars have considered them to be \"tragicomedies\" or otherwise difficult to classify.\n",
    "\n",
    "Although this is only a simple example, it gives a brief glimpse at how computational linguistic techniques may be applied to research in areas you might not think of as having anything to do with computers, such as Shakespeare scholarship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
